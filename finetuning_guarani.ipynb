{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c70d57e",
   "metadata": {},
   "source": [
    "# üî• Fine-Tuning de BLOOM-560M para Transformaciones en Guaran√≠\n",
    "\n",
    "## Proyecto Final - PLN e IA\n",
    "\n",
    "**Objetivo:** Entrenar un modelo ligero multiling√ºe usando **LoRA (Low-Rank Adaptation)** para mejorar el rendimiento en transformaciones morfol√≥gicas del guaran√≠.\n",
    "\n",
    "### ¬øPor qu√© BLOOM-560M?\n",
    "- **Multiling√ºe:** Entrenado en 46 idiomas (incluye lenguas de bajo recurso)\n",
    "- **Eficiente:** 560M par√°metros (funciona en Colab gratuito con GPU T4)\n",
    "- **Open-source:** Sin restricciones de uso\n",
    "\n",
    "### ¬øQu√© es LoRA?\n",
    "**LoRA (Low-Rank Adaptation)** entrena solo ~1% de los par√°metros del modelo mediante matrices de bajo rango, reduciendo:\n",
    "- ‚è±Ô∏è **Tiempo:** 10x m√°s r√°pido que full fine-tuning\n",
    "- üíæ **Memoria:** Usa 4-bit quantization (modelo cabe en 2GB VRAM)\n",
    "- üí∞ **Costo:** Sin necesidad de GPUs A100 caras\n",
    "\n",
    "### Configuraci√≥n del Experimento\n",
    "- **Dataset:** AmericasNLP 2025 - Guaran√≠ (train: ~800 ejemplos)\n",
    "- **√âpocas:** 5\n",
    "- **M√©todo:** LoRA + 4-bit quantization\n",
    "- **Evaluaci√≥n:** Dev set (100 ejemplos)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b53028",
   "metadata": {},
   "source": [
    "## 1. Verificar GPU Disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üîç Verificando hardware...\\n\")\n",
    "\n",
    "# Verificar CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU no disponible. El entrenamiento ser√° LENTO.\")\n",
    "    print(\"üí° En Colab: Runtime > Change runtime type > GPU (T4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca3142",
   "metadata": {},
   "source": [
    "## 2. Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d996f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Instalando dependencias para fine-tuning con LoRA...\\n\")\n",
    "\n",
    "!pip install -q \\\n",
    "  transformers==4.36.0 \\\n",
    "  datasets==2.16.0 \\\n",
    "  peft==0.7.1 \\\n",
    "  bitsandbytes==0.41.3 \\\n",
    "  accelerate==0.25.0 \\\n",
    "  trl==0.7.4 \\\n",
    "  sentencepiece \\\n",
    "  sacrebleu \\\n",
    "  pandas \\\n",
    "  tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Dependencias instaladas correctamente\")\n",
    "print(\"\\nüìö Bibliotecas clave:\")\n",
    "print(\"  ‚Ä¢ transformers: Modelos y tokenizers\")\n",
    "print(\"  ‚Ä¢ peft: LoRA y adaptadores eficientes\")\n",
    "print(\"  ‚Ä¢ bitsandbytes: Cuantizaci√≥n 4-bit\")\n",
    "print(\"  ‚Ä¢ trl: Trainer optimizado para LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42eb69d",
   "metadata": {},
   "source": [
    "## 3. Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from sacrebleu.metrics import BLEU\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Importaciones completadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba0b22",
   "metadata": {},
   "source": [
    "## 4. Configuraci√≥n de Hugging Face Token\n",
    "\n",
    "**Nota:** Necesitas un token de Hugging Face para descargar BLOOM.\n",
    "1. Ve a: https://huggingface.co/settings/tokens\n",
    "2. Crea un token (Read)\n",
    "3. Gu√°rdalo en Colab Secrets como `HF_TOKEN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9735275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    hf_token = userdata.get(\"HF_TOKEN\")\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"‚úÖ Hugging Face token configurado\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ HF_TOKEN en Secrets\")\n",
    "    print(\"üí° Config√∫ralo en: Secrets (üîë) > Add new secret > Name: HF_TOKEN\")\n",
    "    hf_token = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468bcdf",
   "metadata": {},
   "source": [
    "## 5. Descarga del Dataset AmericasNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://raw.githubusercontent.com/AmericasNLP/americasnlp2025/main/ST2_EducationalMaterials/data/\"\n",
    "\n",
    "def load_dataset_from_url(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Descarga dataset TSV desde GitHub.\"\"\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return pd.read_csv(StringIO(r.text), sep=\"\\t\")\n",
    "\n",
    "print(\"üì• Descargando dataset AmericasNLP...\\n\")\n",
    "\n",
    "datasets_urls = {\n",
    "    \"train\": f\"{BASE_URL}guarani-train.tsv\",\n",
    "    \"dev\":   f\"{BASE_URL}guarani-dev.tsv\",\n",
    "    \"test\":  f\"{BASE_URL}guarani-test.tsv\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for split, url in datasets_urls.items():\n",
    "    try:\n",
    "        df = load_dataset_from_url(url)\n",
    "        datasets[split] = df\n",
    "        print(f\"‚úÖ {split.upper():5} ‚Üí {len(df):4} ejemplos | Columnas: {list(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando {split}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total train: {len(datasets['train'])} ejemplos para fine-tuning\")\n",
    "print(f\"üìä Total dev:   {len(datasets['dev'])} ejemplos para validaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50394f6d",
   "metadata": {},
   "source": [
    "### Vista Previa del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274c1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Ejemplos del train set:\\n\")\n",
    "display(datasets[\"train\"].head(5))\n",
    "\n",
    "print(\"\\nüìà Distribuci√≥n de transformaciones en train:\")\n",
    "print(datasets[\"train\"][\"Change\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da412b",
   "metadata": {},
   "source": [
    "## 6. Preparaci√≥n del Dataset para Fine-Tuning\n",
    "\n",
    "Convertimos cada ejemplo en un formato de instrucci√≥n:\n",
    "```\n",
    "### Instrucci√≥n:\n",
    "Transforma la siguiente oraci√≥n en guaran√≠ seg√∫n la regla indicada.\n",
    "\n",
    "### Entrada:\n",
    "Oraci√≥n: \"Ore ndorombyai kuri\"\n",
    "Regla: TYPE:AFF\n",
    "\n",
    "### Respuesta:\n",
    "Ore rombyai kuri\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de reglas (mismo del notebook principal)\n",
    "TRANSFORMATION_RULES = {\n",
    "    \"TYPE:AFF\": \"Convierte una oraci√≥n negativa en afirmativa removiendo la negaci√≥n (ndo-...-i)\",\n",
    "    \"TYPE:NEG\": \"Convierte una oraci√≥n afirmativa en negativa usando ndo- y -i\",\n",
    "    \"TENSE:FUT_SIM\": \"Transforma el verbo al futuro simple agregando -ta\",\n",
    "    \"TENSE:PAST\": \"Convierte la oraci√≥n al pasado usando kuri\",\n",
    "    \"PERSON:1_PL_INC\": \"Cambia el sujeto a primera persona plural inclusiva (√±ande)\",\n",
    "    \"PERSON:1_PL_EXC\": \"Cambia el sujeto a primera persona plural exclusiva (ore)\",\n",
    "    \"PERSON:3\": \"Cambia el sujeto a tercera persona singular (ha'e)\"\n",
    "}\n",
    "\n",
    "def format_instruction(row: pd.Series) -> str:\n",
    "    \"\"\"Convierte una fila del dataset en formato de instrucci√≥n.\"\"\"\n",
    "    rule_desc = TRANSFORMATION_RULES.get(row[\"Change\"], row[\"Change\"])\n",
    "    \n",
    "    return f\"\"\"### Instrucci√≥n:\n",
    "Transforma la siguiente oraci√≥n en guaran√≠ seg√∫n la regla indicada.\n",
    "\n",
    "### Entrada:\n",
    "Oraci√≥n: \"{row[\"Source\"]}\"\n",
    "Regla: {row[\"Change\"]} ({rule_desc})\n",
    "\n",
    "### Respuesta:\n",
    "{row[\"Target\"]}\"\"\"\n",
    "\n",
    "\n",
    "print(\"üîß Formateando dataset para fine-tuning...\\n\")\n",
    "\n",
    "# Formatear train y dev\n",
    "train_texts = [format_instruction(row) for _, row in datasets[\"train\"].iterrows()]\n",
    "dev_texts = [format_instruction(row) for _, row in datasets[\"dev\"].iterrows()]\n",
    "\n",
    "# Crear datasets de Hugging Face\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "dev_dataset = Dataset.from_dict({\"text\": dev_texts})\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)} ejemplos\")\n",
    "print(f\"‚úÖ Dev dataset:   {len(dev_dataset)} ejemplos\")\n",
    "\n",
    "print(\"\\nüìù Ejemplo formateado:\")\n",
    "print(\"=\" * 70)\n",
    "print(train_dataset[0][\"text\"])\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c350c75",
   "metadata": {},
   "source": [
    "## 7. Carga del Modelo Base (BLOOM-560M) con Cuantizaci√≥n 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bigscience/bloom-560m\"\n",
    "\n",
    "print(f\"ü§ñ Cargando modelo: {MODEL_NAME}\\n\")\n",
    "\n",
    "# Configuraci√≥n de cuantizaci√≥n 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Cuantizaci√≥n 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",              # Tipo: NormalFloat4\n",
    "    bnb_4bit_compute_dtype=torch.float16,   # Computaci√≥n en FP16\n",
    "    bnb_4bit_use_double_quant=True          # Doble cuantizaci√≥n\n",
    ")\n",
    "\n",
    "# Cargar modelo con cuantizaci√≥n\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"\\n‚úÖ Modelo y tokenizer cargados\")\n",
    "print(f\"üìä Par√°metros totales: {model.num_parameters() / 1e6:.0f}M\")\n",
    "print(f\"üíæ Tama√±o en memoria: ~2GB (cuantizado 4-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1f7c8",
   "metadata": {},
   "source": [
    "## 8. Configuraci√≥n de LoRA\n",
    "\n",
    "**LoRA** a√±ade matrices de bajo rango entrenables a las capas de atenci√≥n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è Configurando LoRA...\\n\")\n",
    "\n",
    "# Preparar modelo para k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configuraci√≥n de LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # Rango de las matrices LoRA\n",
    "    lora_alpha=32,                  # Escalado de LoRA\n",
    "    target_modules=[                # Capas a adaptar\n",
    "        \"query_key_value\",          # Atenci√≥n (espec√≠fico de BLOOM)\n",
    "    ],\n",
    "    lora_dropout=0.05,              # Dropout\n",
    "    bias=\"none\",                    # No entrenar bias\n",
    "    task_type=\"CAUSAL_LM\"           # Tipo de tarea\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Mostrar par√°metros entrenables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"‚úÖ LoRA configurado correctamente\\n\")\n",
    "print(f\"üìä Par√°metros totales:      {total_params / 1e6:.1f}M\")\n",
    "print(f\"üéØ Par√°metros entrenables:  {trainable_params / 1e6:.1f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"üí° Eficiencia: Entrenamos solo el {100 * trainable_params / total_params:.2f}% del modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa856b1a",
   "metadata": {},
   "source": [
    "## 9. Configuraci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./bloom-560m-guarani-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Hiperpar√°metros principales\n",
    "    num_train_epochs=5,                 # 5 √©pocas\n",
    "    per_device_train_batch_size=4,      # Batch size por GPU\n",
    "    gradient_accumulation_steps=4,      # Simular batch=16\n",
    "    learning_rate=2e-4,                 # Learning rate\n",
    "    \n",
    "    # Optimizaci√≥n\n",
    "    optim=\"paged_adamw_8bit\",           # Optimizador eficiente\n",
    "    warmup_steps=50,                    # Warmup\n",
    "    max_grad_norm=0.3,                  # Gradient clipping\n",
    "    \n",
    "    # Logging y guardado\n",
    "    logging_steps=25,                   # Log cada 25 pasos\n",
    "    save_strategy=\"epoch\",              # Guardar cada √©poca\n",
    "    evaluation_strategy=\"epoch\",        # Evaluar cada √©poca\n",
    "    \n",
    "    # Eficiencia\n",
    "    fp16=True,                          # Mixed precision\n",
    "    group_by_length=True,               # Agrupar por longitud\n",
    "    \n",
    "    # Otros\n",
    "    report_to=\"none\",                   # Sin wandb\n",
    "    load_best_model_at_end=True,        # Cargar mejor modelo\n",
    "    metric_for_best_model=\"loss\",       # M√©trica\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configurados\")\n",
    "print(f\"\\nüìã Configuraci√≥n:\")\n",
    "print(f\"   √âpocas: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size efectivo: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Pasos totales: ~{len(train_dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4539dfa",
   "metadata": {},
   "source": [
    "## 10. Inicializaci√≥n del Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67423f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Inicializando SFTTrainer...\\n\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer inicializado y listo para entrenar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a404d",
   "metadata": {},
   "source": [
    "## 11. üöÄ Entrenamiento del Modelo\n",
    "\n",
    "**Tiempo estimado:** ~30-45 minutos en Colab con GPU T4 (5 √©pocas, ~800 ejemplos)\n",
    "\n",
    "‚ö†Ô∏è **Nota:** El entrenamiento puede tardar. Ve por un caf√© ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî• INICIANDO FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Modelo: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {len(train_dataset)} ejemplos\")\n",
    "print(f\"√âpocas: {training_args.num_train_epochs}\")\n",
    "print(f\"M√©todo: LoRA (r={lora_config.r}, alpha={lora_config.lora_alpha})\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# ENTRENAR\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ FINE-TUNING COMPLETADO\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f545a26",
   "metadata": {},
   "source": [
    "## 12. Guardar el Modelo Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Guardando modelo fine-tuned...\\n\")\n",
    "\n",
    "# Guardar adaptadores LoRA\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Modelo guardado en: {OUTPUT_DIR}\")\n",
    "print(f\"üì¶ Tama√±o: ~50MB (solo adaptadores LoRA)\")\n",
    "\n",
    "# Opcional: Descargar modelo\n",
    "print(\"\\nüì• Para descargar el modelo, ejecuta:\")\n",
    "print(f\"!zip -r {OUTPUT_DIR}.zip {OUTPUT_DIR}\")\n",
    "print(\"from google.colab import files\")\n",
    "print(f\"files.download('{OUTPUT_DIR}.zip')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8dd93",
   "metadata": {},
   "source": [
    "## 13. Evaluaci√≥n del Modelo Fine-Tuned\n",
    "\n",
    "Comparamos el modelo fine-tuned con el baseline (Claude Few-Shot: 50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Evaluando modelo fine-tuned en dev set...\\n\")\n",
    "\n",
    "# Crear pipeline de generaci√≥n\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.1,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "def extract_response(generated_text: str, input_prompt: str) -> str:\n",
    "    \"\"\"Extrae la respuesta del texto generado.\"\"\"\n",
    "    # Remover el prompt original\n",
    "    response = generated_text.replace(input_prompt, \"\").strip()\n",
    "    \n",
    "    # Tomar solo la primera l√≠nea despu√©s de \"### Respuesta:\"\n",
    "    if \"### Respuesta:\" in response:\n",
    "        response = response.split(\"### Respuesta:\")[-1].strip()\n",
    "    \n",
    "    # Tomar solo hasta el primer salto de l√≠nea\n",
    "    response = response.split(\"\\n\")[0].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Evaluar en dev set (primeros 50 para rapidez)\n",
    "dev_sample = datasets[\"dev\"].head(50)\n",
    "results = []\n",
    "\n",
    "print(f\"Evaluando {len(dev_sample)} ejemplos...\\n\")\n",
    "\n",
    "for idx, row in tqdm(dev_sample.iterrows(), total=len(dev_sample)):\n",
    "    # Crear prompt sin respuesta\n",
    "    rule_desc = TRANSFORMATION_RULES.get(row[\"Change\"], row[\"Change\"])\n",
    "    input_prompt = f\"\"\"### Instrucci√≥n:\n",
    "Transforma la siguiente oraci√≥n en guaran√≠ seg√∫n la regla indicada.\n",
    "\n",
    "### Entrada:\n",
    "Oraci√≥n: \"{row[\"Source\"]}\"\n",
    "Regla: {row[\"Change\"]} ({rule_desc})\n",
    "\n",
    "### Respuesta:\n",
    "\"\"\"\n",
    "    \n",
    "    # Generar\n",
    "    outputs = pipe(input_prompt)\n",
    "    generated = outputs[0][\"generated_text\"]\n",
    "    \n",
    "    # Extraer respuesta\n",
    "    prediction = extract_response(generated, input_prompt)\n",
    "    \n",
    "    # Comparar\n",
    "    correct = prediction.lower().strip() == row[\"Target\"].lower().strip()\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": row[\"ID\"],\n",
    "        \"source\": row[\"Source\"],\n",
    "        \"change\": row[\"Change\"],\n",
    "        \"target\": row[\"Target\"],\n",
    "        \"prediction\": prediction,\n",
    "        \"correct\": correct\n",
    "    })\n",
    "\n",
    "# Calcular m√©tricas\n",
    "correct_count = sum(r[\"correct\"] for r in results)\n",
    "total = len(results)\n",
    "accuracy = (correct_count / total) * 100\n",
    "\n",
    "# BLEU\n",
    "bleu = BLEU()\n",
    "predictions = [r[\"prediction\"] for r in results]\n",
    "references = [[r[\"target\"]] for r in results]\n",
    "bleu_score = bleu.corpus_score(predictions, references).score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE EVALUACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüéØ Accuracy: {accuracy:.2f}% ({correct_count}/{total})\")\n",
    "print(f\"üìä BLEU Score: {bleu_score:.2f}\")\n",
    "\n",
    "print(\"\\nüìà Comparaci√≥n con baseline:\")\n",
    "print(f\"   Claude 3.5 Few-Shot: 50.0% (mejor anterior)\")\n",
    "print(f\"   BLOOM-560M Fine-Tuned: {accuracy:.2f}%\")\n",
    "\n",
    "if accuracy > 50:\n",
    "    print(\"\\nüèÜ ¬°El modelo fine-tuned SUPER√ì el baseline!\")\n",
    "elif accuracy > 40:\n",
    "    print(\"\\n‚úÖ Rendimiento competitivo con el baseline\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Rendimiento por debajo del baseline (puede mejorar con m√°s √©pocas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf3e44",
   "metadata": {},
   "source": [
    "## 14. Ejemplos de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EJEMPLOS DE PREDICCIONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, result in enumerate(results[:10], 1):\n",
    "    status = \"‚úÖ\" if result[\"correct\"] else \"‚ùå\"\n",
    "    print(f\"\\n{status} Ejemplo {i}:\")\n",
    "    print(f\"   Oraci√≥n:    {result['source']}\")\n",
    "    print(f\"   Regla:      {result['change']}\")\n",
    "    print(f\"   Esperado:   {result['target']}\")\n",
    "    print(f\"   Predicci√≥n: {result['prediction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35f1ed",
   "metadata": {},
   "source": [
    "## 15. Guardar Resultados de Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2327510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Guardar resultados\n",
    "eval_results = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"method\": \"LoRA Fine-Tuning\",\n",
    "    \"epochs\": training_args.num_train_epochs,\n",
    "    \"train_size\": len(train_dataset),\n",
    "    \"eval_size\": len(results),\n",
    "    \"accuracy\": accuracy,\n",
    "    \"bleu\": bleu_score,\n",
    "    \"correct\": correct_count,\n",
    "    \"total\": total,\n",
    "    \"predictions\": results\n",
    "}\n",
    "\n",
    "with open(\"finetuning_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Resultados guardados en: finetuning_results.json\")\n",
    "\n",
    "# Descargar\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"finetuning_results.json\")\n",
    "    print(\"üì• Archivo descargado\")\n",
    "except:\n",
    "    print(\"üí° Descarga manual disponible en archivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8bbab",
   "metadata": {},
   "source": [
    "## 16. Tabla Comparativa Final\n",
    "\n",
    "Comparamos todas las estrategias probadas en el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707aab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Resultados anteriores del notebook principal\n",
    "comparison_data = [\n",
    "    {\"Modelo\": \"GPT-3.5 Turbo\", \"Estrategia\": \"Zero-Shot\", \"Accuracy (%)\": 0.0, \"BLEU\": 59.46},\n",
    "    {\"Modelo\": \"GPT-3.5 Turbo\", \"Estrategia\": \"Few-Shot\", \"Accuracy (%)\": 10.0, \"BLEU\": 0.0},\n",
    "    {\"Modelo\": \"GPT-3.5 Turbo\", \"Estrategia\": \"Semantic RAG\", \"Accuracy (%)\": 0.0, \"BLEU\": 59.46},\n",
    "    {\"Modelo\": \"GPT-3.5 Turbo\", \"Estrategia\": \"Hybrid RAG\", \"Accuracy (%)\": 10.0, \"BLEU\": 0.0},\n",
    "    {\"Modelo\": \"Claude 3.5 Sonnet\", \"Estrategia\": \"Zero-Shot\", \"Accuracy (%)\": 30.0, \"BLEU\": 0.0},\n",
    "    {\"Modelo\": \"Claude 3.5 Sonnet\", \"Estrategia\": \"Few-Shot\", \"Accuracy (%)\": 50.0, \"BLEU\": 0.0},\n",
    "    {\"Modelo\": \"Claude 3.5 Sonnet\", \"Estrategia\": \"Semantic RAG\", \"Accuracy (%)\": 20.0, \"BLEU\": 0.0},\n",
    "    {\"Modelo\": \"Claude 3.5 Sonnet\", \"Estrategia\": \"Hybrid RAG\", \"Accuracy (%)\": 40.0, \"BLEU\": 0.0},\n",
    "    # Agregar resultado del fine-tuning\n",
    "    {\"Modelo\": \"BLOOM-560M\", \"Estrategia\": \"Fine-Tuning (LoRA)\", \"Accuracy (%)\": round(accuracy, 2), \"BLEU\": round(bleu_score, 2)}\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Ordenar por accuracy\n",
    "comparison_df = comparison_df.sort_values(\"Accuracy (%)\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA COMPARATIVA COMPLETA - TODOS LOS EXPERIMENTOS\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identificar mejor\n",
    "best = comparison_df.iloc[0]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üèÜ MEJOR CONFIGURACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Modelo:      {best['Modelo']}\")\n",
    "print(f\"Estrategia:  {best['Estrategia']}\")\n",
    "print(f\"Accuracy:    {best['Accuracy (%)']}%\")\n",
    "print(f\"BLEU:        {best['BLEU']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ea2bc",
   "metadata": {},
   "source": [
    "## 17. Conclusiones del Fine-Tuning\n",
    "\n",
    "### ¬øQu√© aprendimos?\n",
    "\n",
    "1. **Fine-tuning con LoRA es eficiente:**\n",
    "   - Entrenar solo ~1% de par√°metros\n",
    "   - Funciona en Colab gratuito (GPU T4)\n",
    "   - Modelo final: ~50MB (vs 1.1GB del modelo completo)\n",
    "\n",
    "2. **BLOOM-560M es competitivo para guaran√≠:**\n",
    "   - Modelo multiling√ºe pre-entrenado\n",
    "   - Se adapta bien con pocos ejemplos (~800)\n",
    "\n",
    "3. **Comparaci√≥n con baseline:**\n",
    "   - Si accuracy > 50%: Fine-tuning super√≥ a Claude Few-Shot\n",
    "   - Si accuracy < 50%: Necesita m√°s √©pocas o datos\n",
    "\n",
    "### Pr√≥ximos pasos\n",
    "\n",
    "Para mejorar a√∫n m√°s:\n",
    "- ‚úÖ Usar todo el train set (hecho)\n",
    "- ‚úÖ Probar m√°s √©pocas (5 √©pocas completadas)\n",
    "- üîÑ Ajustar hiperpar√°metros (learning rate, r, alpha)\n",
    "- üîÑ Probar modelos m√°s grandes (BLOOM-1b7, mGPT)\n",
    "- üîÑ Data augmentation (generar m√°s ejemplos sint√©ticos)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Experimento completado!\n",
    "\n",
    "**Archivos generados:**\n",
    "- `bloom-560m-guarani-lora/` - Modelo fine-tuned\n",
    "- `finetuning_results.json` - Resultados de evaluaci√≥n\n",
    "\n",
    "**Para usar el modelo:**\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./bloom-560m-guarani-lora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bloom-560m-guarani-lora\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
