{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3ac083",
   "metadata": {},
   "source": [
    "# Sistema de Transformaci√≥n de Oraciones en Guaran√≠ con RAG\n",
    "\n",
    "## Proyecto Final - PLN e IA\n",
    "\n",
    "**Objetivo:** Implementar un sistema capaz de transformar oraciones en guaran√≠ seg√∫n reglas gramaticales espec√≠ficas, comparando el rendimiento de:\n",
    "- 2 modelos de LLM (GPT-3.5 Turbo vs Claude 3.5 Sonnet)\n",
    "- 2 estrategias: **Sin RAG** (conocimiento del modelo) vs **Con RAG** (recuperaci√≥n de documentaci√≥n gramatical)\n",
    "\n",
    "## Dataset: AmericasNLP 2025\n",
    "- **Task:** Transformaci√≥n de oraciones en guaran√≠\n",
    "- **Input:** `Source` (oraci√≥n base) + `Change` (tipo de transformaci√≥n)\n",
    "- **Output:** `Target` (oraci√≥n transformada)\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Source: \"Ore ndorombyai kuri\"\n",
    "Change: \"TYPE:AFF\" (convertir a afirmativo)\n",
    "Target: \"Ore rombyai kuri\"\n",
    "```\n",
    "\n",
    "## Metodolog√≠a\n",
    "1. Cargar dataset AmericasNLP (train/dev/test)\n",
    "2. Construir base de conocimiento (RAG) con gram√°tica guaran√≠\n",
    "3. Implementar sistema de transformaci√≥n con y sin RAG\n",
    "4. Evaluar con m√©tricas objetivas (BLEU, accuracy, etc.)\n",
    "5. Comparar resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d478c8",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Instalaci√≥n de librer√≠as\n",
    "!pip install -q -U \\\n",
    "  langchain langchain-core langchain-community langchain-text-splitters \\\n",
    "  sentence-transformers faiss-cpu \\\n",
    "  requests python-dotenv pymupdf pandas \\\n",
    "  sacrebleu nltk\n",
    "\n",
    "print(\"‚úì Dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f1755",
   "metadata": {},
   "source": [
    "## 2. Importaciones y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import time\n",
    "\n",
    "# LangChain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# M√©tricas\n",
    "from sacrebleu.metrics import BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "print(\"‚úì Importaciones completadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012fbbc",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n de API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d0773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar OpenRouter API Key\n",
    "# Opci√≥n 1: Usar Colab Secrets (recomendado)\n",
    "# Opci√≥n 2: Ingresar manualmente\n",
    "\n",
    "if 'OPENROUTER_API_KEY' in globals():\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
    "    print(\"‚úì API Key cargada desde variable del kernel\")\n",
    "else:\n",
    "    from getpass import getpass\n",
    "    api_key = getpass(\"Ingresa tu OPENROUTER_API_KEY: \")\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = api_key\n",
    "    print(\"‚úì API Key configurada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d829a5e",
   "metadata": {},
   "source": [
    "## 4. Descarga del Dataset AmericasNLP\n",
    "\n",
    "Dataset oficial: https://github.com/AmericasNLP/americasnlp2025/tree/main/ST2_EducationalMaterials/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs del dataset\n",
    "BASE_URL = \"https://raw.githubusercontent.com/AmericasNLP/americasnlp2025/main/ST2_EducationalMaterials/data/\"\n",
    "\n",
    "datasets_urls = {\n",
    "    \"train\": f\"{BASE_URL}guarani-train.tsv\",\n",
    "    \"dev\": f\"{BASE_URL}guarani-dev.tsv\",\n",
    "    \"test\": f\"{BASE_URL}guarani-test.tsv\"\n",
    "}\n",
    "\n",
    "def load_dataset(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Carga el dataset desde URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Guardar temporalmente\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Leer con pandas\n",
    "    df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "# Cargar datasets\n",
    "print(\"Descargando datasets...\\n\")\n",
    "datasets = {}\n",
    "\n",
    "for split, url in datasets_urls.items():\n",
    "    try:\n",
    "        df = load_dataset(url)\n",
    "        datasets[split] = df\n",
    "        print(f\"‚úì {split.upper()}: {len(df)} ejemplos\")\n",
    "        print(f\"  Columnas: {list(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando {split}: {e}\")\n",
    "\n",
    "# Vista previa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EJEMPLO DEL DATASET (primeras 3 filas de DEV):\")\n",
    "print(\"=\"*60)\n",
    "print(datasets[\"dev\"].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37797eec",
   "metadata": {},
   "source": [
    "## 5. Carga de Gram√°tica Guaran√≠ (Base de Conocimiento para RAG)\n",
    "\n",
    "**Nota:** Sube el archivo `Gram√°tica guaran√≠.pdf` a Colab o usa Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opci√≥n 1: Subir archivo manualmente\n",
    "from google.colab import files\n",
    "print(\"Por favor, sube el archivo 'Gram√°tica guaran√≠.pdf'\")\n",
    "uploaded = files.upload()\n",
    "PDF_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Opci√≥n 2: Desde Google Drive (descomentar si prefieres esta opci√≥n)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PDF_PATH = '/content/drive/MyDrive/Gram√°tica guaran√≠.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad40bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extrae texto del PDF de gram√°tica\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    print(f\"üìò Extrayendo texto de: {pdf_path}\")\n",
    "    print(f\"   Total de p√°ginas: {len(doc)}\")\n",
    "    \n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        page_text = page.get_text()\n",
    "        text += f\"\\n--- P√°gina {page_num} ---\\n{page_text}\"\n",
    "        \n",
    "        if page_num % 10 == 0:\n",
    "            print(f\"   ‚Ä¢ Procesadas: {page_num} p√°ginas\")\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"‚úì Extracci√≥n completada: {len(text)} caracteres\")\n",
    "    return text.strip()\n",
    "\n",
    "# Extraer texto\n",
    "pdf_text = extract_text_from_pdf(PDF_PATH)\n",
    "\n",
    "# Vista previa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MUESTRA DEL TEXTO (primeros 500 caracteres):\")\n",
    "print(\"=\"*60)\n",
    "print(pdf_text[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bcd20",
   "metadata": {},
   "source": [
    "## 6. Construcci√≥n del Vector Store (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757305bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir texto en chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(\"üìö Dividiendo texto en chunks...\")\n",
    "text_chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Crear documentos\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=chunk,\n",
    "        metadata={\"source\": \"Gram√°tica guaran√≠\", \"chunk_id\": idx}\n",
    "    )\n",
    "    for idx, chunk in enumerate(text_chunks)\n",
    "]\n",
    "\n",
    "print(f\"‚úì Creados {len(documents)} chunks\")\n",
    "print(f\"  Tama√±o promedio: {sum(len(c) for c in text_chunks) / len(text_chunks):.0f} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e8f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de embeddings\n",
    "print(\"üîÑ Cargando modelo de embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "print(\"‚úì Modelo de embeddings listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vector store con FAISS\n",
    "print(\"üíæ Creando vector store...\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Configurar retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Top 3 documentos m√°s relevantes\n",
    ")\n",
    "\n",
    "print(\"‚úì Vector store creado\")\n",
    "print(\"‚úì Retriever configurado (k=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be306c70",
   "metadata": {},
   "source": [
    "## 7. Wrapper para OpenRouter (GPT-3.5 y Claude 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenRouterLLM:\n",
    "    \"\"\"Wrapper para usar modelos via OpenRouter\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    def generate(self, prompt: str, max_tokens: int = 200, temperature: float = 0.3) -> str:\n",
    "        \"\"\"Genera respuesta del modelo\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://colab.research.google.com/\",\n",
    "            \"X-Title\": \"guarani-transformation\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.api_url, headers=headers, json=data, timeout=60)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                return f\"Error {response.status_code}: {response.text}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "# Configurar modelos\n",
    "api_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "model_gpt35 = OpenRouterLLM(\"openai/gpt-3.5-turbo\", api_key)\n",
    "model_claude = OpenRouterLLM(\"anthropic/claude-3.5-sonnet\", api_key)\n",
    "\n",
    "print(\"‚úì Modelos configurados:\")\n",
    "print(\"  - GPT-3.5 Turbo (OpenAI)\")\n",
    "print(\"  - Claude 3.5 Sonnet (Anthropic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac718f",
   "metadata": {},
   "source": [
    "## 8. Sistema de Transformaci√≥n de Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a96494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuaraniTransformationSystem:\n",
    "    \"\"\"Sistema de transformaci√≥n de oraciones en guaran√≠\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, retriever=None):\n",
    "        self.llm = llm\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def transform_without_rag(self, source: str, change: str) -> str:\n",
    "        \"\"\"Transformaci√≥n SIN RAG (solo conocimiento del modelo)\"\"\"\n",
    "        prompt = f\"\"\"Eres un experto en el idioma guaran√≠.\n",
    "\n",
    "Transforma la siguiente oraci√≥n en guaran√≠ seg√∫n la regla indicada.\n",
    "\n",
    "Oraci√≥n original: \"{source}\"\n",
    "Regla de transformaci√≥n: {change}\n",
    "\n",
    "Responde √öNICAMENTE con la oraci√≥n transformada en guaran√≠, sin explicaciones adicionales.\n",
    "\n",
    "Oraci√≥n transformada:\"\"\"\n",
    "        \n",
    "        return self.llm.generate(prompt)\n",
    "    \n",
    "    def transform_with_rag(self, source: str, change: str) -> str:\n",
    "        \"\"\"Transformaci√≥n CON RAG (con contexto gramatical)\"\"\"\n",
    "        # Recuperar contexto relevante\n",
    "        query = f\"transformaci√≥n {change} en guaran√≠ negaci√≥n afirmaci√≥n\"\n",
    "        docs = self.retriever.invoke(query)\n",
    "        context = \"\\n\\n\".join([d.page_content[:600] for d in docs[:3]])\n",
    "        \n",
    "        prompt = f\"\"\"Eres un experto en el idioma guaran√≠.\n",
    "\n",
    "Usa la siguiente informaci√≥n gramatical para transformar la oraci√≥n:\n",
    "\n",
    "CONTEXTO GRAMATICAL:\n",
    "{context}\n",
    "\n",
    "TAREA:\n",
    "Oraci√≥n original: \"{source}\"\n",
    "Regla de transformaci√≥n: {change}\n",
    "\n",
    "INSTRUCCIONES:\n",
    "- Basa tu transformaci√≥n en las reglas gramaticales del contexto\n",
    "- Responde √öNICAMENTE con la oraci√≥n transformada en guaran√≠\n",
    "- NO agregues explicaciones\n",
    "\n",
    "Oraci√≥n transformada:\"\"\"\n",
    "        \n",
    "        return self.llm.generate(prompt)\n",
    "    \n",
    "    def evaluate_dataset(self, df: pd.DataFrame, use_rag: bool = False) -> List[Dict]:\n",
    "        \"\"\"Eval√∫a el sistema en un dataset completo\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            source = row[\"Source\"]\n",
    "            change = row[\"Change\"]\n",
    "            target = row[\"Target\"]\n",
    "            \n",
    "            # Generar predicci√≥n\n",
    "            if use_rag:\n",
    "                prediction = self.transform_with_rag(source, change)\n",
    "            else:\n",
    "                prediction = self.transform_without_rag(source, change)\n",
    "            \n",
    "            results.append({\n",
    "                \"id\": row[\"ID\"],\n",
    "                \"source\": source,\n",
    "                \"change\": change,\n",
    "                \"target\": target,\n",
    "                \"prediction\": prediction,\n",
    "                \"correct\": prediction.strip().lower() == target.strip().lower()\n",
    "            })\n",
    "            \n",
    "            # Peque√±a pausa para no saturar la API\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úì Sistema de transformaci√≥n implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61f37c",
   "metadata": {},
   "source": [
    "## 9. Evaluaci√≥n de Modelos\n",
    "\n",
    "Se evaluar√°n ambos modelos (GPT-3.5 y Claude 3.5) con y sin RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e46c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n en DEV set (usamos DEV primero para ajustar, luego TEST para resultados finales)\n",
    "dev_data = datasets[\"dev\"].head(10)  # Primero probamos con 10 ejemplos\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUANDO MODELO: GPT-3.5 TURBO SIN RAG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "system_gpt_no_rag = GuaraniTransformationSystem(model_gpt35, retriever=None)\n",
    "results_gpt_no_rag = system_gpt_no_rag.evaluate_dataset(dev_data, use_rag=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUANDO MODELO: GPT-3.5 TURBO CON RAG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "system_gpt_rag = GuaraniTransformationSystem(model_gpt35, retriever=retriever)\n",
    "results_gpt_rag = system_gpt_rag.evaluate_dataset(dev_data, use_rag=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUANDO MODELO: CLAUDE 3.5 SONNET SIN RAG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "system_claude_no_rag = GuaraniTransformationSystem(model_claude, retriever=None)\n",
    "results_claude_no_rag = system_claude_no_rag.evaluate_dataset(dev_data, use_rag=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUANDO MODELO: CLAUDE 3.5 SONNET CON RAG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "system_claude_rag = GuaraniTransformationSystem(model_claude, retriever=retriever)\n",
    "results_claude_rag = system_claude_rag.evaluate_dataset(dev_data, use_rag=True)\n",
    "\n",
    "print(\"\\n‚úì Evaluaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b12ed2",
   "metadata": {},
   "source": [
    "## 10. C√°lculo de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(results: List[Dict]) -> Dict:\n",
    "    \"\"\"Calcula m√©tricas de evaluaci√≥n\"\"\"\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r[\"correct\"])\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    # BLEU score\n",
    "    bleu = BLEU()\n",
    "    references = [[r[\"target\"]] for r in results]\n",
    "    predictions = [r[\"prediction\"] for r in results]\n",
    "    bleu_score = bleu.corpus_score(predictions, references).score\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"correct\": correct,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"bleu\": bleu_score\n",
    "    }\n",
    "\n",
    "# Calcular m√©tricas para cada configuraci√≥n\n",
    "metrics = {\n",
    "    \"GPT-3.5 Sin RAG\": calculate_metrics(results_gpt_no_rag),\n",
    "    \"GPT-3.5 Con RAG\": calculate_metrics(results_gpt_rag),\n",
    "    \"Claude 3.5 Sin RAG\": calculate_metrics(results_claude_no_rag),\n",
    "    \"Claude 3.5 Con RAG\": calculate_metrics(results_claude_rag)\n",
    "}\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS DE EVALUACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for config, metrics_data in metrics.items():\n",
    "    print(f\"\\n{config}:\")\n",
    "    print(f\"  Accuracy: {metrics_data['accuracy']:.2f}%\")\n",
    "    print(f\"  BLEU Score: {metrics_data['bleu']:.2f}\")\n",
    "    print(f\"  Correctas: {metrics_data['correct']}/{metrics_data['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f411f7d",
   "metadata": {},
   "source": [
    "## 11. Tabla Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff190af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame comparativo\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Modelo\": [\"GPT-3.5 Turbo\", \"GPT-3.5 Turbo\", \"Claude 3.5 Sonnet\", \"Claude 3.5 Sonnet\"],\n",
    "    \"Estrategia\": [\"Sin RAG\", \"Con RAG\", \"Sin RAG\", \"Con RAG\"],\n",
    "    \"Accuracy (%)\": [\n",
    "        metrics[\"GPT-3.5 Sin RAG\"][\"accuracy\"],\n",
    "        metrics[\"GPT-3.5 Con RAG\"][\"accuracy\"],\n",
    "        metrics[\"Claude 3.5 Sin RAG\"][\"accuracy\"],\n",
    "        metrics[\"Claude 3.5 Con RAG\"][\"accuracy\"]\n",
    "    ],\n",
    "    \"BLEU Score\": [\n",
    "        metrics[\"GPT-3.5 Sin RAG\"][\"bleu\"],\n",
    "        metrics[\"GPT-3.5 Con RAG\"][\"bleu\"],\n",
    "        metrics[\"Claude 3.5 Sin RAG\"][\"bleu\"],\n",
    "        metrics[\"Claude 3.5 Con RAG\"][\"bleu\"]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA COMPARATIVA\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Encontrar mejor configuraci√≥n\n",
    "best_idx = comparison_df[\"Accuracy (%)\"].idxmax()\n",
    "best_config = comparison_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEJOR CONFIGURACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Modelo: {best_config['Modelo']}\")\n",
    "print(f\"Estrategia: {best_config['Estrategia']}\")\n",
    "print(f\"Accuracy: {best_config['Accuracy (%)']:.2f}%\")\n",
    "print(f\"BLEU: {best_config['BLEU Score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af82e7",
   "metadata": {},
   "source": [
    "## 12. Ejemplos de Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0863e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar algunos ejemplos de transformaciones\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EJEMPLOS DE TRANSFORMACIONES (GPT-3.5 CON RAG)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, result in enumerate(results_gpt_rag[:5], 1):\n",
    "    print(f\"\\nEjemplo {i}:\")\n",
    "    print(f\"  ID: {result['id']}\")\n",
    "    print(f\"  Oraci√≥n original: {result['source']}\")\n",
    "    print(f\"  Transformaci√≥n: {result['change']}\")\n",
    "    print(f\"  Esperado: {result['target']}\")\n",
    "    print(f\"  Generado: {result['prediction']}\")\n",
    "    print(f\"  ‚úì Correcto\" if result['correct'] else \"  ‚úó Incorrecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba05dec",
   "metadata": {},
   "source": [
    "## 13. Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados completos\n",
    "all_results = {\n",
    "    \"gpt35_no_rag\": results_gpt_no_rag,\n",
    "    \"gpt35_rag\": results_gpt_rag,\n",
    "    \"claude_no_rag\": results_claude_no_rag,\n",
    "    \"claude_rag\": results_claude_rag,\n",
    "    \"metrics\": metrics,\n",
    "    \"metadata\": {\n",
    "        \"dataset\": \"AmericasNLP 2025 - Guaran√≠\",\n",
    "        \"models\": [\"openai/gpt-3.5-turbo\", \"anthropic/claude-3.5-sonnet\"],\n",
    "        \"strategies\": [\"Sin RAG\", \"Con RAG\"],\n",
    "        \"embedding_model\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        \"vector_store\": \"FAISS\",\n",
    "        \"grammar_source\": \"Gram√°tica guaran√≠.pdf\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar en JSON\n",
    "with open(\"guarani_transformation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úì Resultados guardados en: guarani_transformation_results.json\")\n",
    "\n",
    "# Guardar tabla comparativa en CSV\n",
    "comparison_df.to_csv(\"comparison_table.csv\", index=False)\n",
    "print(\"‚úì Tabla comparativa guardada en: comparison_table.csv\")\n",
    "\n",
    "# Descargar archivos (solo en Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"guarani_transformation_results.json\")\n",
    "    files.download(\"comparison_table.csv\")\n",
    "    print(\"\\n‚úì Archivos descargados\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è No est√°s en Colab. Archivos guardados localmente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309bd28",
   "metadata": {},
   "source": [
    "## 14. Conclusiones\n",
    "\n",
    "### Preguntas Clave a Responder:\n",
    "\n",
    "1. **¬øQu√© modelo es mejor?**\n",
    "   - Analizar accuracy y BLEU score\n",
    "   - Considerar velocidad y costo\n",
    "\n",
    "2. **¬øRAG mejora el rendimiento?**\n",
    "   - Comparar m√©tricas con/sin RAG\n",
    "   - Analizar ejemplos donde RAG ayuda\n",
    "\n",
    "3. **¬øPor qu√© estos resultados?**\n",
    "   - Guaran√≠ es idioma de bajo recursos\n",
    "   - RAG aporta conocimiento gramatical espec√≠fico\n",
    "   - Modelos grandes (Claude) vs r√°pidos (GPT-3.5)\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "- Evaluar en TEST set completo\n",
    "- Ajustar prompts seg√∫n resultados\n",
    "- Considerar fine-tuning si hay datos suficientes\n",
    "- Agregar m√°s documentaci√≥n gramatical al vector store"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
